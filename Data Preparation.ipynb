{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc226cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the library\n",
    "\n",
    "# to generate data processing and visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# clustering and dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f999b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "drug_reviews_drugs_com = fetch_ucirepo(id=462)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a813f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "df = drug_reviews_drugs_com.data.features\n",
    "\n",
    "# view dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1616ccd",
   "metadata": {},
   "source": [
    "# **Data Preparation**\n",
    "\n",
    "During the data preparation, several steps will be carried out to retrieve a cleaned dataset for further analysis. <br>\n",
    "The steps are:\n",
    "1. Identify Missing Value\n",
    "2. Remove Duplicates\n",
    "3. Convert Date Format\n",
    "4. Lowercasing the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c89df",
   "metadata": {},
   "source": [
    "## **Handling Missing Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the uniqueness of condition\n",
    "len(df.condition.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most presence condition in the dataset\n",
    "df.condition.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with \"No Specified\"\n",
    "# reason: have a clear flag for analysis\n",
    "# and not confident that drug performance is reflecting the most occurrence condition\n",
    "df.condition.fillna('Not Specified', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a90d03",
   "metadata": {},
   "source": [
    "## **Handling Irrelevant Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as shown in the EDA, condition consists of irrelavent data\n",
    "# replace with Not Specified\n",
    "# the common word: </span> users found this comment helpful\n",
    "# using regex to search for the words then replace with 'Not Specified'\n",
    "df['condition'] = df['condition'].replace(r'\\d+</span> users found this comment helpful\\.', 'Not Specified', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the \"Not Specified\" was filled to the missing condition column\n",
    "df[df['condition'] == 'Not Specified'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if missing values still exist\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2c96d",
   "metadata": {},
   "source": [
    "## **Handling Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c08cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the duplicates\n",
    "duplicates = df.duplicated()\n",
    "df[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dimension of dataset\n",
    "print(df.shape)\n",
    "\n",
    "print(f\"The dataset consists of\",df.shape[0], \"drug reviews from patient.\")\n",
    "print(f\"The dataset consists of the features\",', '.join(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42df80",
   "metadata": {},
   "source": [
    "## **Lowercase Column Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a28fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the text\n",
    "df['drugName'] = df['drugName'].str.lower()\n",
    "df['condition'] = df['condition'].str.lower()\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dimension of cleaned dataset:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download csv file\n",
    "df.to_csv('cleaned dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14721dca",
   "metadata": {},
   "source": [
    "# **Best Clustering Approach Used**\n",
    "\n",
    "There are three most popular clustering approaches such as KMeans, Hierarchical Clustering and DBSCAN.\n",
    "In order to determine which clustering approach is the best for this experiment, a few steps had been taken. <br>\n",
    "\n",
    "**Two steps to determine are:**\n",
    "1. Outlier Detection\n",
    "2. Compare Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025c47c",
   "metadata": {},
   "source": [
    "## **Outlier Detection**\n",
    "\n",
    "PCA diagram showed that **majority of data points were packed together** in the central region while there **exist with some data points that is spread away** from the central region. These data points which distribute from central region can be considered as the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a777f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection_df = pd.read_csv('cleaned dataset.csv')\n",
    "outlier_detection_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204868ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize reviews using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(outlier_detection_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef729f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# visualize results for outlier determination\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "plt.title('PCA with 2 Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f96ff",
   "metadata": {},
   "source": [
    "## **Clustering Techniques Performance**\n",
    "\n",
    "DBSCAN outperformed k-means and agglomerative hierarchical. These initial findings provide an initial study for determining the clustering approach that will be applied after data derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = outlier_detection_df.copy()\n",
    "\n",
    "# vectorize reviews using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(cluster_df['cleaned review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling to get subset of data\n",
    "# obtained 10% data from dataset\n",
    "\n",
    "# get number of rows\n",
    "n_rows = tfidf_matrix.shape[0]\n",
    "\n",
    "# 10% percent of data\n",
    "sample_size = int(0.1 * n_rows)\n",
    "\n",
    "# random select rows from dataset\n",
    "random_indices = np.random.choice(n_rows, size=sample_size, replace=False)\n",
    "\n",
    "# subset of data\n",
    "random_data = tfidf_matrix[random_indices]\n",
    "random_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the silhouette scores\n",
    "# find the optimal number of clusters using silhouette score\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "# apply kmeans\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(random_data)\n",
    "    score = silhouette_score(random_data, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# polt\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title(f'Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# the highest score\n",
    "optimal_k = range(2, 11)[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9707b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans clustering\n",
    "kmeans = KMeans(n_clusters=9)\n",
    "kmeans_labels = kmeans.fit_predict(random_data)\n",
    "\n",
    "# evaluate clustering\n",
    "sil_score_kmeans = silhouette_score(random_data, kmeans_labels)\n",
    "print(f\"Silhouette Score for K-Means: {sil_score_kmeans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d75810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agglomerative clustering\n",
    "agglo = AgglomerativeClustering(n_clusters=9)\n",
    "agglo_labels = agglo.fit_predict(random_data.toarray())\n",
    "\n",
    "# evaluate clustering\n",
    "sil_score_agglo = silhouette_score(random_data, agglo_labels)\n",
    "print(f\"Silhouette Score for Agglomerative Clustering: {sil_score_agglo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd146c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan clustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# store the best parameters\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "\n",
    "# range to iterate\n",
    "eps_values = [0.1, 0.5, 1.0, 1.3, 1.5]\n",
    "min_samples_values = [10, 50, 100, 500, 1000]\n",
    "\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan.fit_predict(random_data)\n",
    "\n",
    "        num_clusters = len(np.unique(dbscan_labels))\n",
    "        if num_clusters > 1:\n",
    "            # calculate silhouette score\n",
    "            score = silhouette_score(random_data, dbscan_labels)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_eps = eps\n",
    "                best_min_samples = min_samples\n",
    "\n",
    "# show results\n",
    "print(f\"Best Silhouette Score: {best_score}\")\n",
    "print(f\"Best eps: {best_eps}\")\n",
    "print(f\"Best min_samples: {best_min_samples}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
