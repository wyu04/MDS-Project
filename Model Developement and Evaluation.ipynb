{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the library\n",
    "\n",
    "# to generate data processing and visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# parsing and timing utilities\n",
    "import ast\n",
    "import time\n",
    "\n",
    "# openAI API integration\n",
    "import openai\n",
    "import tiktoken\n",
    "from openai import OpenAI, RateLimitError\n",
    "\n",
    "# test processing and normalization\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# clustering and dimensionality reduction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# monitoring\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd199a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the cleaned dataset\n",
    "drug_reviews = pd.read_csv('cleaned dataset.csv')\n",
    "drug_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null and duplication\n",
    "print(\"The number of missing rows are: \", drug_reviews.isnull().sum().sum())\n",
    "print(\"The number of duplicated rows are: \", drug_reviews.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a342b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing columns with blank \"\"\n",
    "drug_reviews.fillna(\"\", inplace=True)\n",
    "\n",
    "# check the missing values\n",
    "print(\"The number of missing rows are: \", drug_reviews.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732455c9",
   "metadata": {},
   "source": [
    "# **Sample Filtering**\n",
    "\n",
    "In this section, demonstrate how the **dataset is filtered** based on multiple criteria to prepare subsets for clustering analysis. The filtering process includes **thresholds on review usefulness, ratings, and minimum frequency of drugs and conditions**. This helps to focus on the most relevant and representative reviews, reducing noise and improving clustering quality.\n",
    "\n",
    "After applying these filters, the dataset is further divided into **positive** and **negative** sentiment groups based on the review polarity scores. This division allows us to analyze patterns specific to sentiment categories.\n",
    "\n",
    "Due to the size and complexity of the filtered datasets, applying the DBSCAN clustering algorithm on both subsets is computationally intensive and requires approximately 24 hours to complete on available hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b69390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, \n",
    "                min_useful_count=None, \n",
    "                min_ratings=None, \n",
    "                min_drug_frequency=None, \n",
    "                min_condition_frequency=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Filters the drug reviews DataFrame based on given criteria:\n",
    "    - Minimum usefulCount (number of helpful votes)\n",
    "    - Minimum rating threshold\n",
    "    - Minimum frequency for drugs to keep\n",
    "    - Minimum frequency for conditions to keep\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with drug reviews.\n",
    "        min_useful_count (int, optional): Minimum usefulCount to keep reviews.\n",
    "        min_ratings (float or int, optional): Minimum rating to keep reviews.\n",
    "        min_drug_frequency (int, optional): Minimum number of times a drug must appear.\n",
    "        min_condition_frequency (int, optional): Minimum number of times a condition must appear.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame after applying all criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_drug_reviews = drug_reviews.copy()\n",
    "    \n",
    "    if min_useful_count is not None:\n",
    "        filtered_drug_reviews = filtered_drug_reviews[filtered_drug_reviews['usefulCount'] >= min_useful_count]\n",
    "        print(f\"Filtered reviews with usefulCount >= {min_useful_count}: {filtered_drug_reviews.shape[0]} reviews\")\n",
    "\n",
    "    if min_ratings is not None:\n",
    "        filtered_drug_reviews = filtered_drug_reviews[filtered_drug_reviews['rating'] > min_ratings]\n",
    "        print(f\"Filtered reviews with valid ratings {min_ratings}: {filtered_drug_reviews.shape[0]} reviews\")\n",
    "    \n",
    "    if min_drug_frequency is not None:\n",
    "        drug_counts = filtered_drug_reviews['drugName'].value_counts()\n",
    "        frequent_drugs = drug_counts[drug_counts > min_drug_frequency].index\n",
    "        filtered_drug_reviews = filtered_drug_reviews[filtered_drug_reviews['drugName'].isin(frequent_drugs)]\n",
    "        print(f\"Filtered reviews with drugs appearing more than {min_drug_frequency} times: {filtered_drug_reviews.shape[0]} reviews\")\n",
    "\n",
    "    if min_condition_frequency is not None:\n",
    "        cond_counts = filtered_drug_reviews['condition'].value_counts()\n",
    "        frequent_conds = cond_counts[cond_counts > min_condition_frequency].index\n",
    "        filtered_drug_reviews = filtered_drug_reviews[filtered_drug_reviews['condition'].isin(frequent_conds)]\n",
    "        print(f\"Filtered reviews with conditions appearing more than {min_condition_frequency} times: {filtered_drug_reviews.shape[0]} reviews\")\n",
    "\n",
    "    return filtered_drug_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a39678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the parameters\n",
    "print(\"usefulCount statistics:\")\n",
    "print(drug_reviews['usefulCount'].describe())\n",
    "print()\n",
    "print(\"Rating statistics:\")\n",
    "print(drug_reviews['rating'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e982f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_drug_reviews = filter_data(drug_reviews, min_useful_count=16, min_ratings=5,\n",
    "                          min_drug_frequency=1, min_condition_frequency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimension of filtered dataset:\", filtered_drug_reviews.shape)\n",
    "filtered_drug_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_drug_reviews['Sentiment Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further divide the dataset into 2 categories - positive sentiment and negative sentiment\n",
    "positive_reviews = filtered_drug_reviews[filtered_drug_reviews['Sentiment Category'] == 2]\n",
    "negative_reviews = filtered_drug_reviews[filtered_drug_reviews['Sentiment Category'] == 0]\n",
    "\n",
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "print(\"The dimension of negative reviews:\", negative_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = positive_reviews[['drugName', 'condition', 'review', 'rating', 'usefulCount', 'side effects', 'effectiveness']]\n",
    "positive_reviews = positive_reviews.reset_index(drop=True)\n",
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = negative_reviews[['drugName', 'condition', 'review', 'rating', 'usefulCount', 'side effects', 'effectiveness']]\n",
    "negative_reviews = negative_reviews.reset_index(drop=True)\n",
    "print(\"The dimension of negative reviews:\", negative_reviews.shape)\n",
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbe7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download positive reviews and negative reviews\n",
    "positive_reviews.to_csv('positive reviews.csv', index=False)\n",
    "negative_reviews.to_csv('negative reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d76051",
   "metadata": {},
   "source": [
    "# **Using TF-IDF to Vectorize**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd34b7",
   "metadata": {},
   "source": [
    "## **Tokenizaion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ddfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c561e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to map POS tags from nltk.pos_tag to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert POS tag from nltk.pos_tag to a format compatible with WordNetLemmatizer.\n",
    "    WordNetLemmatizer requires POS tags like ADJ, VERB, NOUN, ADV to lemmatize accurately.\n",
    "    \n",
    "    Args:\n",
    "        tag (str): POS tag from nltk.pos_tag\n",
    "    \n",
    "    Returns:\n",
    "        wordnet constant: corresponding WordNet POS tag\n",
    "    \"\"\"\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0220e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to normalize the text\n",
    "# remove punctuation, lowercase, remove stopwords, lemmatization\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize the input text by:\n",
    "    - converting to lowercase\n",
    "    - removing punctuation and numbers (keeping only letters and spaces)\n",
    "    - tokenizing into words\n",
    "    - applying POS-tag based lemmatization to retain word meanings\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text to be normalized\n",
    "    \n",
    "    Returns:\n",
    "        str: Normalized and lemmatized text as a single string\n",
    "    \"\"\"\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    # remove character that is not a lowercase letter or space\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # lemmatization\n",
    "    # to hold the meanings of the words\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure the text properly transformed before normalization\n",
    "# check if the text is a string or list\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process input text to ensure compatibility with normalization function.\n",
    "    Supports input as string or list of strings.\n",
    "    \n",
    "    Args:\n",
    "        text (str or list): Text input to be normalized\n",
    "    \n",
    "    Returns:\n",
    "        str: Normalized text string\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the text is a string\n",
    "    if isinstance(text, str):\n",
    "        return normalize_text(text)\n",
    "    \n",
    "    # check if the text is a list\n",
    "    elif isinstance(text, list):\n",
    "        return normalize_text(' '.join(text))\n",
    "    \n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data (df):\n",
    "    \"\"\"\n",
    "    Perform basic data quality checks on the given DataFrame.\n",
    "    \n",
    "    Checks include:\n",
    "    - Counting total missing values across all columns\n",
    "    - Counting total duplicated rows\n",
    "    - Reporting the shape (rows, columns) of the dataset\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input dataset to check\n",
    "    \n",
    "    Prints:\n",
    "        Number of missing values, duplicated rows, and dataset dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for missing values\n",
    "    print(\"The number of missing rows are: \", df.isnull().sum().sum())\n",
    "    \n",
    "    # check for duplicates\n",
    "    print(\"The number of duplicated rows are: \", df.duplicated().sum())\n",
    "    \n",
    "    # check the dimension of the dataset\n",
    "    print(\"The dimension of the dataset is: \", df.shape)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f046ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['normalized_side_effects'] = positive_reviews['side effects'].apply(process_text)\n",
    "negative_reviews['normalized_side_effects'] = negative_reviews['side effects'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['normalized_side_effects'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60875ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews['normalized_side_effects'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b800fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['normalized_effectiveness'] = positive_reviews['effectiveness'].apply(process_text)\n",
    "negative_reviews['normalized_effectiveness'] = negative_reviews['effectiveness'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['normalized_effectiveness'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11046c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews['normalized_effectiveness'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c55616",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data(positive_reviews)\n",
    "check_data(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ef26c",
   "metadata": {},
   "source": [
    "## **Unique Side Effects and Effectiveness Observed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text to visualize the frequency of words for side effects and effectiveness\n",
    "\n",
    "def tokenize(X):\n",
    "    \"\"\"\n",
    "    Tokenize the text data by:\n",
    "    - Converting to lowercase\n",
    "    - Removing specific punctuation\n",
    "    - Splitting text by commas\n",
    "    - Removing tokens with more than 5 words (likely phrases)\n",
    "    - Removing empty tokens\n",
    "\n",
    "    Args:\n",
    "        X (pd.Series): Series containing text data to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        list: List of cleaned tokens/phrases\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert to lowercase\n",
    "    X = X.str.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    X = X.str.replace(r\"\\[\", \"\", regex=True)\n",
    "    X = X.str.replace(r\"\\]\", \"\", regex=True)\n",
    "    X = X.str.replace(r\"\\(\", \"\", regex=True)\n",
    "    X = X.str.replace(r\"\\)\", \"\", regex=True)\n",
    "    X = X.str.replace(\"'\", \"\", regex=True)\n",
    "\n",
    "    # tokenize\n",
    "    tokenize_word = []\n",
    "    for i in range(len(X)):\n",
    "        # split the content by comma\n",
    "        word = X[i].split(',')\n",
    "        # remove whitespace and skip for the phrase with more than 5 words\n",
    "        word = [element.strip() for element in word if len(element.split()) <= 5]\n",
    "        tokenize_word = tokenize_word + word\n",
    "\n",
    "    # remove empty content\n",
    "    tokenize_word = [x for x in tokenize_word if x != \"\"]\n",
    "\n",
    "    return tokenize_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a80fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(df):\n",
    "    \"\"\"\n",
    "    Generate and visualize frequency counts for 'side effects' and 'effectiveness' columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'side effects' and 'effectiveness' columns\n",
    "    \n",
    "    Displays:\n",
    "        - Counts of total and unique tokens in both categories\n",
    "        - Bar plots for the top 10 tokens by frequency in each category\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize the text\n",
    "    side_effects = tokenize(df['side effects'])\n",
    "    effectiveness = tokenize(df['effectiveness'])\n",
    "\n",
    "    # get the word count\n",
    "    print(\"The number of side effects: \", len(side_effects))\n",
    "    print(\"The number of effectiveness: \", len(effectiveness))\n",
    "\n",
    "    # turn the list into dataframe\n",
    "    side_effects_df = pd.DataFrame(side_effects, columns=['side_effects'])\n",
    "    effectiveness_df = pd.DataFrame(effectiveness, columns=['effectiveness'])\n",
    "\n",
    "    # get the frequency of each word\n",
    "    side_effects_count = side_effects_df['side_effects'].value_counts()\n",
    "    effectiveness_count = effectiveness_df['effectiveness'].value_counts()\n",
    "    print(\"The number of unique side effects: \", len(side_effects_count))\n",
    "    print(\"The number of unique effectiveness: \", len(effectiveness_count))\n",
    "    \n",
    "    # sort the word count\n",
    "    side_effect_df = side_effects_count.reset_index()\n",
    "    side_effect_df.columns = ['Side Effect', 'Count']\n",
    "    side_effect_df = side_effect_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "    effectiveness_df = effectiveness_count.reset_index()\n",
    "    effectiveness_df.columns = ['Effectiveness', 'Count']\n",
    "    effectiveness_df = effectiveness_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "    # plot the top 10 side effects\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "    # for first top 10 side effects found in the dataset\n",
    "    sns.barplot(x='Count', y='Side Effect', data=side_effect_df.head(10), palette='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Top 10 Most Common Side Effects')    \n",
    "    axes[0].set_xlabel('Count')\n",
    "    axes[0].set_ylabel('Side Effect')\n",
    "\n",
    "    # for first top 10 effectiveness found in the dataset\n",
    "    sns.barplot(x='Count', y='Effectiveness', data=effectiveness_df.head(10), palette='coolwarm', ax=axes[1])\n",
    "    axes[1].set_title('Top 10 Most Common Effectiveness')\n",
    "    axes[1].set_xlabel('Count')\n",
    "    axes[1].set_ylabel('Effectiveness')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97700831",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_count(positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff79e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_count(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8adfe",
   "metadata": {},
   "source": [
    "## **Vectorization by TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_df(df, column_name):\n",
    "    \"\"\"\n",
    "    Prints the number of features extracted with varying `min_df` values using TF-IDF vectorization.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the text data.\n",
    "    - column_name (str): Name of the column containing text.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    for min_df in [1, 5, 10, 15, 20]:\n",
    "        # vectorize the text\n",
    "        tfidf = TfidfVectorizer(min_df=min_df, stop_words='english', max_df = 0.95)\n",
    "        tfidf_matrix = tfidf.fit_transform(df[column_name])\n",
    "\n",
    "        # get the shape of the matrix\n",
    "        print(f\"min_df={min_df}: n_features={len(tfidf.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(df, column_name, min_df):    \n",
    "    \"\"\"\n",
    "    Vectorizes the text in a specified column of a DataFrame using TF-IDF.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - column_name (str): The name of the column containing text data.\n",
    "    - min_df (int): The minimum number of documents a term must appear in to be included.\n",
    "\n",
    "    Returns:\n",
    "    - tfidf_matrix (sparse matrix): The TF-IDF representation of the text data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # vectorize the text\n",
    "    tfidf = TfidfVectorizer(min_df=min_df, stop_words='english', max_df = 0.95)\n",
    "    tfidf_matrix = tfidf.fit_transform(df[column_name])\n",
    "\n",
    "    # get the shape of the matrix\n",
    "    print(f\"n_samples: {tfidf_matrix.shape[0]}, n_features: {tfidf_matrix.shape[1]}\", '\\n')\n",
    "\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3cd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tf_idf (df, side_effect_tfidf, effectiveness_tfidf):\n",
    "    \"\"\"\n",
    "    Checks the TF-IDF matrices and the input DataFrame for basic data integrity.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The original DataFrame.\n",
    "    - side_effect_tfidf (sparse matrix): TF-IDF matrix for side effects.\n",
    "    - effectiveness_tfidf (sparse matrix): TF-IDF matrix for effectiveness.\n",
    "\n",
    "    Prints:\n",
    "    - Number of missing values and duplicated rows in the DataFrame.\n",
    "    - Dimensions of the side effects and effectiveness TF-IDF matrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for missing values\n",
    "    print(\"The number of missing rows are: \", df.isnull().sum().sum())\n",
    "    \n",
    "    # check for duplicates\n",
    "    print(\"The number of duplicated rows are: \", df.duplicated().sum())\n",
    "    \n",
    "    # check the dimension of the side effects\n",
    "    print(\"The dimension of side effects tf_idf is: \", side_effect_tfidf.shape)\n",
    "\n",
    "    # check the dimension of the effectiveness\n",
    "    print(\"The dimension of effectiveness tf_idf is: \", effectiveness_tfidf.shape)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For side effects in positive reviews:\")\n",
    "get_min_df(positive_reviews, 'normalized_side_effects')\n",
    "print()\n",
    "print(\"For effectiveness in positive reviews:\")\n",
    "get_min_df(positive_reviews, 'normalized_effectiveness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For side effects in negative reviews:\")\n",
    "get_min_df(negative_reviews, 'normalized_side_effects')\n",
    "print()\n",
    "print(\"For effectiveness in negative reviews:\")\n",
    "get_min_df(negative_reviews, 'normalized_effectiveness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For side effects in positive review:\")\n",
    "side_effects_positive_tf_idf = vectorize_text(positive_reviews, 'normalized_side_effects', 10)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"For effectiveness in positive review:\")\n",
    "effectiveness_positive_tf_idf = vectorize_text(positive_reviews, 'normalized_effectiveness', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed147c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For side effects in negative review:\")\n",
    "side_effects_negative_tf_idf = vectorize_text(negative_reviews, 'normalized_side_effects', 10)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"For effectiveness in negative review:\")\n",
    "effectiveness_negative_tf_idf = vectorize_text(negative_reviews, 'normalized_effectiveness', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09902f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For positive reviews\")\n",
    "check_tf_idf(positive_reviews, side_effects_positive_tf_idf, effectiveness_positive_tf_idf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"For negative reviews\")\n",
    "check_tf_idf(negative_reviews, side_effects_negative_tf_idf, effectiveness_negative_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc301140",
   "metadata": {},
   "source": [
    "## **Normalization/Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd39440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(tfidf_matrix):\n",
    "    \"\"\"\n",
    "    Applies L2 normalization to the TF-IDF matrix so that each row (document vector) \n",
    "    has a unit norm. This ensures cosine similarity can be used properly in clustering or distance-based methods.\n",
    "\n",
    "    Parameters:\n",
    "    - tfidf_matrix (sparse matrix): The TF-IDF matrix to normalize.\n",
    "\n",
    "    Returns:\n",
    "    - normalized_data (sparse matrix): L2-normalized TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize the data\n",
    "    normalizer = Normalizer()\n",
    "    normalized_data = normalizer.fit_transform(tfidf_matrix)\n",
    "\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_side_effects_scaled = normalization(side_effects_positive_tf_idf)\n",
    "positive_effectiveness_scaled = normalization(effectiveness_positive_tf_idf)\n",
    "\n",
    "print(\"Shape of positive side effects scaled data: \", positive_side_effects_scaled.shape)\n",
    "print(\"Shape of positive effectiveness scaled data: \", positive_effectiveness_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83302928",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_side_effects_scaled = normalization(side_effects_negative_tf_idf)\n",
    "negative_effectiveness_scaled = normalization(effectiveness_negative_tf_idf)\n",
    "\n",
    "print(\"Shape of negative side effects scaled data: \", negative_side_effects_scaled.shape)\n",
    "print(\"Shape of negative effectiveness scaled data: \", negative_effectiveness_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea8dc0",
   "metadata": {},
   "source": [
    "## **Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d21103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(data_scaled, name, step, max_components):\n",
    "    \"\"\"\n",
    "    Plots the cumulative explained variance as a function of the number of components \n",
    "    for Truncated SVD (suitable for sparse data like TF-IDF matrices).\n",
    "\n",
    "    Parameters:\n",
    "    - data_scaled (sparse matrix or ndarray): The normalized or scaled feature matrix.\n",
    "    - name (str): Label for the dataset or variable (used in print and plot titles).\n",
    "    - step (int): Step size to increment the number of components.\n",
    "    - max_components (int): Maximum number of components to consider.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays a line plot showing cumulative explained variance.\n",
    "    \"\"\"\n",
    "\n",
    "    explained_variance_ratios = []\n",
    "\n",
    "    n_components_range = list(range(1, max_components, step))\n",
    "\n",
    "    for n in n_components_range:\n",
    "        svd = TruncatedSVD(n_components=n)\n",
    "        svd.fit_transform(data_scaled)\n",
    "        total_variance = np.sum(svd.explained_variance_ratio_)\n",
    "        explained_variance_ratios.append(total_variance)\n",
    "        print(f\"[{name}] Components: {n}, Cumulative Explained Variance: {total_variance:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(n_components_range, explained_variance_ratios, marker='o')\n",
    "    plt.title(f\"Cumulative Explained Variance vs Number of Components for {name}\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction(data_scaled, n_components):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction using Truncated SVD (suitable for sparse matrices).\n",
    "\n",
    "    Parameters:\n",
    "    - data_scaled (sparse matrix or ndarray): Normalized or scaled input data (e.g., TF-IDF).\n",
    "    - n_components (int): Number of dimensions to reduce the data to.\n",
    "\n",
    "    Returns:\n",
    "    - reduced_data (ndarray): Transformed dataset with reduced dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    reduced_data = svd.fit_transform(data_scaled)\n",
    "\n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0db35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Positive Reviews\")\n",
    "plot_explained_variance(positive_side_effects_scaled, \"Positive Side Effects\", 500, 2000)\n",
    "print()\n",
    "plot_explained_variance(positive_effectiveness_scaled, \"Positive Effectiveness\", 500, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25624b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Negative Reviews\")\n",
    "plot_explained_variance(negative_side_effects_scaled, \"Negative Side Effects\", 300, 1200)\n",
    "print()\n",
    "plot_explained_variance(negative_effectiveness_scaled, \"Negative Effectiveness\", 400, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e381f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_svd_side_effects = dimensionality_reduction(positive_side_effects_scaled, 1000)\n",
    "positive_svd_effectiveness = dimensionality_reduction(positive_effectiveness_scaled, 1500)\n",
    "print(\"Shape of positive side effects SVD data: \", positive_svd_side_effects.shape)\n",
    "print(\"Shape of positive effectiveness SVD data: \", positive_svd_effectiveness.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_svd_side_effects = dimensionality_reduction(negative_side_effects_scaled, 800)\n",
    "negative_svd_effectiveness = dimensionality_reduction(negative_effectiveness_scaled, 1000)\n",
    "print(\"Shape of negative side effects SVD data: \", negative_svd_side_effects.shape)\n",
    "print(\"Shape of negative effectiveness SVD data: \", negative_svd_effectiveness.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abaa6d",
   "metadata": {},
   "source": [
    "## **DBSCAN Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the epsilon value\n",
    "# using cosine because when data is high dimensional, euclidean distance is not effective\n",
    "# algorithm default should be auto, but here we use brute force\n",
    "# reason is that cosine distance only support by brute force\n",
    "\n",
    "def plot_k_distance(data, k):\n",
    "    \"\"\"\n",
    "    Plots the k-distance graph to help determine the optimal epsilon for DBSCAN.\n",
    "\n",
    "    Parameters:\n",
    "    - data: ndarray or sparse matrix\n",
    "    - k_values: list of integers representing the k-nearest neighbors to test\n",
    "    - metric: distance metric to use (default is 'cosine')\n",
    "    \"\"\"\n",
    "    \n",
    "    for k_values in k:\n",
    "        print(\"k-distance for k = \", k_values)\n",
    "        neigh = NearestNeighbors(n_neighbors=k_values, metric='cosine', algorithm='brute')\n",
    "        neigh.fit(data)\n",
    "        distances, _ = neigh.kneighbors(data)\n",
    "\n",
    "        sorted_distances = np.sort(distances[:, -1])\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(sorted_distances)\n",
    "        plt.xlabel(\"Points sorted by distance\")\n",
    "        plt.ylabel(f\"{k_values}-th Nearest Neighbor Distance\")\n",
    "        plt.title(\"k-Distance Graph to Find Optimal Eps\")\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e746f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dbscan(X, eps_values, min_samples_values):\n",
    "    \"\"\"\n",
    "    Tunes DBSCAN parameters and returns the best configuration based on DBI.\n",
    "\n",
    "    Parameters:\n",
    "    - X: ndarray or sparse matrix\n",
    "    - eps_values: list of epsilon values\n",
    "    - min_samples_values: list of min_samples values\n",
    "    - metric: distance metric for DBSCAN (default is 'cosine')\n",
    "\n",
    "    Returns:\n",
    "    - best_eps: best epsilon value found\n",
    "    - best_score: lowest DBI score\n",
    "    - best_min_samples: corresponding min_samples value\n",
    "    \"\"\"\n",
    "\n",
    "    best_eps = None\n",
    "    best_score = float('inf')\n",
    "    best_min_samples = None\n",
    "\n",
    "    for min_samples in min_samples_values:\n",
    "        for eps in eps_values:\n",
    "            print(f\"Testing min_samples = {min_samples}, eps = {eps}\")\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "            labels = dbscan.fit_predict(X)\n",
    "\n",
    "            print(\"Done Clustering\")\n",
    "\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "            if n_clusters >= 2:\n",
    "\n",
    "                # exclude noise points\n",
    "                sil_score = silhouette_score(X[labels!= -1], labels[labels!=-1])\n",
    "                print(f\"Silhouette Coefficients: {sil_score:.4f}\")\n",
    "                db_index = davies_bouldin_score(X[labels!= -1], labels[labels!=-1])\n",
    "                print(f\"DBI: {db_index:.4f}\")\n",
    "                \n",
    "                if db_index < best_score:\n",
    "                    best_score = db_index\n",
    "                    best_eps = eps\n",
    "                    best_min_samples = min_samples\n",
    "            \n",
    "                # how many clusters formed\n",
    "                print(\"Clustered labels: \", n_clusters)\n",
    "                # how many number of noise points\n",
    "                print(\"Noise points: \", list(labels).count(-1))\n",
    "            \n",
    "            else:\n",
    "                print(\"No clusters formed or all points are noise.\")\n",
    "            \n",
    "        print()\n",
    "\n",
    "    return best_eps, best_score, best_min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_insert_labels(df, features, eps, min_samples, label_column_name):\n",
    "    \"\"\"\n",
    "    Applies DBSCAN clustering and inserts the cluster labels into the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: original pandas DataFrame\n",
    "    - features: features to cluster on (e.g., reduced TF-IDF)\n",
    "    - eps: epsilon value for DBSCAN\n",
    "    - min_samples: minimum samples for DBSCAN\n",
    "    - label_column_name: name of the column to store cluster labels\n",
    "    - metric: distance metric for DBSCAN (default is 'cosine')\n",
    "\n",
    "    Returns:\n",
    "    - df: updated DataFrame with cluster labels\n",
    "    \"\"\"\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "    labels = dbscan.fit_predict(features)\n",
    "\n",
    "    # insert labels into the original DataFrame\n",
    "    df[label_column_name] = labels\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    if n_clusters >= 2:\n",
    "\n",
    "        # exclude noise points\n",
    "        sil_score = silhouette_score(features[labels!=-1], labels[labels!=-1])\n",
    "        print(f\"Silhouette Coefficients: {sil_score:.4f}\")\n",
    "        db_index = davies_bouldin_score(features[labels!=-1], labels[labels!=-1])\n",
    "        print(f\"DBI: {db_index:.4f}\")\n",
    "\n",
    "        # how many clusters formed\n",
    "        print(\"Clustered labels: \", len(set(labels)) - 1)\n",
    "        # how many number of noise points\n",
    "        print(\"Noise points: \", list(labels).count(-1))\n",
    "        \n",
    "    else:\n",
    "        print(f\"[{label_column_name}] Too few clusters to compute DBI\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b4d84",
   "metadata": {},
   "source": [
    "### **Positive Side Effects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d38efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(positive_svd_side_effects, k = [10, 100, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_svd_side_effects, eps_values = [0.1, 0.2, 0.3, 0.6, 0.75, 0.85], min_samples_values = [10, 100, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_svd_side_effects, eps_values = [0.4, 0.5], min_samples_values = [100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select eps with 0.1 and min_samples with 100\n",
    "# compact and well-separated clusters with relatively low dbi\n",
    "\n",
    "positive_reviews = cluster_and_insert_labels(\n",
    "    df=positive_reviews,\n",
    "    features=positive_svd_side_effects,\n",
    "    eps=0.1,\n",
    "    min_samples=100,\n",
    "    label_column_name='side_effects_labels'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af82fb",
   "metadata": {},
   "source": [
    "### **Positive Effectiveness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(positive_svd_effectiveness, k = [10, 100, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_svd_effectiveness, eps_values = [0.2, 0.4, 0.5, 0.7, 0.8, 0.85], min_samples_values = [10, 100, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_svd_effectiveness, eps_values = [0.25, 0.3, 0.35], min_samples_values = [20, 30, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = cluster_and_insert_labels(\n",
    "    df=positive_reviews,\n",
    "    features=positive_svd_effectiveness,\n",
    "    eps=0.3,\n",
    "    min_samples=20,\n",
    "    label_column_name='effectiveness_labels'\n",
    ")\n",
    "\n",
    "# positive_reviews.to_csv('positive reviews with labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fdf28d",
   "metadata": {},
   "source": [
    "### **Negative Side Effects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f100fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(negative_svd_side_effects, k = [10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_svd_side_effects, eps_values = [0.15, 0.4, 0.5, 0.7, 0.85], min_samples_values = [10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_svd_side_effects, eps_values = [0.2, 0.3, 0.4, 0.5], min_samples_values = [50, 70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faff846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eps = 0.22 and min_samples = 30\n",
    "negative_reviews = cluster_and_insert_labels(\n",
    "    df=negative_reviews,\n",
    "    features=negative_svd_side_effects,\n",
    "    eps=0.5,\n",
    "    min_samples=50,\n",
    "    label_column_name='side_effects_labels'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ac490",
   "metadata": {},
   "source": [
    "### **Negative Effectiveness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49963672",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(negative_svd_effectiveness, k = [10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_svd_effectiveness, eps_values = [0.2, 0.5, 0.6, 0.7, 0.85], min_samples_values = [10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_svd_effectiveness, eps_values = [0.5, 0.55, 0.6, 0.65], min_samples_values = [5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01511c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = cluster_and_insert_labels(\n",
    "    df=negative_reviews,\n",
    "    features=negative_svd_effectiveness,\n",
    "    eps=0.5,\n",
    "    min_samples=10,\n",
    "    label_column_name='effectiveness_labels'\n",
    ")\n",
    "\n",
    "# negative_reviews.to_csv('negative reviews with labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59455c",
   "metadata": {},
   "source": [
    "## **Join Two Dataset Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure the text is in list format\n",
    "def safe_parse_list(x):\n",
    "    \"\"\"\n",
    "    Safely parses a string representation of a list into a Python list.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input value that may be a string representation of a list.\n",
    "    \n",
    "    Returns:\n",
    "    - A parsed list if successful.\n",
    "    - A list containing the original string if parsing fails.\n",
    "    - An empty list if the input is not a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return [x]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the columns\n",
    "positive_reviews = positive_reviews[['drugName', 'condition', 'review', 'rating', 'usefulCount', 'side effects', 'side_effects_labels', 'effectiveness', 'effectiveness_labels']]\n",
    "negative_reviews = negative_reviews[['drugName', 'condition', 'review', 'rating', 'usefulCount', 'side effects', 'side_effects_labels', 'effectiveness', 'effectiveness_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdca54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the sentiment column\n",
    "positive_reviews['sentiment'] = \"Positive\"\n",
    "negative_reviews['sentiment'] = \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfcb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the positive and negative reviews\n",
    "drug_reviews = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
    "drug_reviews = drug_reviews.reset_index(drop=True)\n",
    "\n",
    "# drop \"review\" column\n",
    "drug_reviews = drug_reviews.drop(columns=['review'])\n",
    "print(\"The dimension of drug reviews:\", drug_reviews.shape)\n",
    "drug_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_reviews['side effects'] = drug_reviews['side effects'].apply(\n",
    "    lambda x: ' '.join(str(item) for item in safe_parse_list(x))\n",
    ")\n",
    "\n",
    "drug_reviews['effectiveness'] = drug_reviews['effectiveness'].apply(\n",
    "    lambda x: ' '.join(str(item) for item in safe_parse_list(x))\n",
    ")\n",
    "\n",
    "drug_reviews['side effects'] = drug_reviews['side effects'].replace('', 'no side effects reported')\n",
    "drug_reviews['effectiveness'] = drug_reviews['effectiveness'].replace('', 'no effectiveness information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea678bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "drug_reviews.to_csv('drug reviews with labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638d4f1",
   "metadata": {},
   "source": [
    "# **Text Embedding with OpenAI ada-002**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure the text is in list format\n",
    "def safe_parse_list(x):\n",
    "    \"\"\"\n",
    "    Safely parses a string representation of a list into a Python list.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input value that may be a string representation of a list.\n",
    "    \n",
    "    Returns:\n",
    "    - A parsed list if successful.\n",
    "    - A list containing the original string if parsing fails.\n",
    "    - An empty list if the input is not a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return [x]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432000ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = pd.read_csv(\"positive reviews.csv\")\n",
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['side effects'] = positive_reviews['side effects'].apply(\n",
    "    lambda x: ' '.join(str(item) for item in safe_parse_list(x))\n",
    ")\n",
    "\n",
    "positive_reviews['effectiveness'] = positive_reviews['effectiveness'].apply(\n",
    "    lambda x: ' '.join(str(item) for item in safe_parse_list(x))\n",
    ")\n",
    "\n",
    "positive_reviews['side effects'] = positive_reviews['side effects'].replace('', 'no side effects reported')\n",
    "positive_reviews['effectiveness'] = positive_reviews['effectiveness'].replace('', 'no effectiveness information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423651ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3586508",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = pd.read_csv(\"negative reviews.csv\")\n",
    "print(\"The dimension of positive reviews:\", negative_reviews.shape)\n",
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da97414",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews['side effects'] = negative_reviews['side effects'].apply(\n",
    "    lambda x: ' '.join(str(item) for item in safe_parse_list(x))\n",
    ")\n",
    "\n",
    "negative_reviews['effectiveness'] = negative_reviews['effectiveness'].apply(\n",
    "    lambda x: ' '.join(str(item) for item in safe_parse_list(x))\n",
    ")\n",
    "\n",
    "negative_reviews['side effects'] = negative_reviews['side effects'].replace('', 'no side effects reported')\n",
    "negative_reviews['effectiveness'] = negative_reviews['effectiveness'].replace('', 'no effectiveness information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab97f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "print(\"The missing values in positive reviews:\", positive_reviews.isnull().sum().sum())\n",
    "\n",
    "print(\"The dimension of negative reviews:\", negative_reviews.shape)\n",
    "print(\"The missing values in negative reviews:\", negative_reviews.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d024ffc",
   "metadata": {},
   "source": [
    "## **Call API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ab356",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade openai\n",
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call API key\n",
    "client = OpenAI(api_key=\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd7a12",
   "metadata": {},
   "source": [
    "## **Tiktoken**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c97d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the model\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "\n",
    "# Function to count tokens in a text\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a given text using the tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of tokens in the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the text to get the token IDs\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    # Return the number of tokens\n",
    "    # as v2-ada model has a limit of 8191 tokens\n",
    "    return len(tokens) <= 8191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7821ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows where both columns meet the token limit\n",
    "positive_reviews = positive_reviews[positive_reviews['side effects'].apply(count_tokens)].reset_index(drop=True)\n",
    "positive_reviews = positive_reviews[positive_reviews['effectiveness'].apply(count_tokens)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee31f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows where both columns meet the token limit\n",
    "negative_reviews = negative_reviews[negative_reviews['side effects'].apply(count_tokens)].reset_index(drop=True)\n",
    "negative_reviews = negative_reviews[negative_reviews['effectiveness'].apply(count_tokens)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc959bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimension of negative reviews:\", negative_reviews.shape)\n",
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c9c8b",
   "metadata": {},
   "source": [
    "## **Text Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845face",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text embedding\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate an embedding vector for the given text using OpenAI's embedding API.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): Input text to embed.\n",
    "\n",
    "    Returns:\n",
    "    - list[float] or None: Embedding vector as a list of floats, or None if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "\n",
    "        # store the embedding\n",
    "        embeded = response.data[0].embedding\n",
    "\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    except RateLimitError:\n",
    "        print(\"Rate limit hit. Sleeping for 10 seconds...\")\n",
    "        time.sleep(10)\n",
    "        return get_embedding(text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['side effects embedding'] = positive_reviews['side effects'].progress_apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['effectiveness embedding'] = positive_reviews['effectiveness'].progress_apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ec31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for side effects\n",
    "negative_reviews['side effects embedding'] = negative_reviews['side effects'].progress_apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4af7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for effectiveness\n",
    "negative_reviews['effectiveness embedding'] = negative_reviews['effectiveness'].progress_apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd486a2",
   "metadata": {},
   "source": [
    "## **Process Text Embedding for Future Use**\n",
    "As text embedding return as the string data type, hence it is not suitable for model developement. <br>\n",
    "Therefore, string data type will be converted into list and further transform into array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the string representation of lists back to actual lists\n",
    "positive_reviews['side effects embedding'] = positive_reviews['side effects embedding'].progress_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b89bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['effectiveness embedding'] = positive_reviews['effectiveness embedding'].progress_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews['side effects embedding'] = negative_reviews['side effects embedding'].progress_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce03544",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews['effectiveness embedding'] = negative_reviews['effectiveness embedding'].progress_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_side_effects = np.vstack(positive_reviews['side effects embedding'].values)\n",
    "positive_effectiveness = np.vstack(positive_reviews['effectiveness embedding'].values)\n",
    "\n",
    "negative_side_effects = np.vstack(negative_reviews['side effects embedding'].values)\n",
    "negative_effectiveness = np.vstack(negative_reviews['effectiveness embedding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac395e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Positive Reviews: \")\n",
    "print(f\"For side effects: {positive_side_effects.shape}\")\n",
    "print(positive_side_effects)\n",
    "print()\n",
    "print(f\"For effectiveness: {positive_effectiveness.shape}\")\n",
    "print(positive_effectiveness)\n",
    "\n",
    "print()\n",
    "print(\"--\" * 20)\n",
    "print()\n",
    "\n",
    "print(\"For Negative Reviews: \")\n",
    "print(f\"For side effects: {negative_side_effects.shape}\")\n",
    "print(negative_side_effects)\n",
    "print()\n",
    "print(f\"For effectiveness: {negative_effectiveness.shape}\")\n",
    "print(negative_effectiveness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41f450",
   "metadata": {},
   "source": [
    "## **DBSCAN Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8942871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced dimensionality\n",
    "def plot_explained_variance(data_scaled, name, step, max_components):\n",
    "    \"\"\"\n",
    "    Plot cumulative explained variance ratio vs. number of components using TruncatedSVD.\n",
    "\n",
    "    Parameters:\n",
    "    - data_scaled: array-like, shape (n_samples, n_features)\n",
    "        The normalized data matrix.\n",
    "    - name: str\n",
    "        Name identifier for the dataset (used in plot titles and print statements).\n",
    "    - step: int\n",
    "        Step size for the number of components to test.\n",
    "    - max_components: int\n",
    "        Maximum number of components to test.\n",
    "    \"\"\"\n",
    "\n",
    "    explained_variance_ratios = []\n",
    "\n",
    "    n_components_range = list(range(1, max_components, step))\n",
    "\n",
    "    for n in n_components_range:\n",
    "        svd = TruncatedSVD(n_components=n)\n",
    "        svd.fit_transform(data_scaled)\n",
    "        total_variance = np.sum(svd.explained_variance_ratio_)\n",
    "        explained_variance_ratios.append(total_variance)\n",
    "        print(f\"[{name}] Components: {n}, Cumulative Explained Variance: {total_variance:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(n_components_range, explained_variance_ratios, marker='o')\n",
    "    plt.title(f\"Cumulative Explained Variance vs Number of Components for {name}\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8689383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction(data_scaled, n_components):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction using TruncatedSVD.\n",
    "\n",
    "    Parameters:\n",
    "    - data_scaled: array-like, shape (n_samples, n_features)\n",
    "        The normalized data matrix.\n",
    "    - n_components: int\n",
    "        Number of components to keep.\n",
    "\n",
    "    Returns:\n",
    "    - reduced_data: array-like, shape (n_samples, n_components)\n",
    "        Dimensionally reduced data.\n",
    "    \"\"\"\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    reduced_data = svd.fit_transform(data_scaled)\n",
    "    print(f\"Reduced data shape: {reduced_data.shape}\")\n",
    "\n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_distance(data, k):\n",
    "    \"\"\"\n",
    "    Plot k-distance graphs for different values of k to help determine DBSCAN's epsilon.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array-like, shape (n_samples, n_features)\n",
    "        Data for which k-distance is computed.\n",
    "    - k: list of int\n",
    "        List of k values (number of nearest neighbors) to plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    for k_values in k:\n",
    "        print(\"k-distance for k = \", k_values)\n",
    "        neigh = NearestNeighbors(n_neighbors=k_values, metric='cosine', algorithm='brute')\n",
    "        neigh.fit(data)\n",
    "        distances, _ = neigh.kneighbors(data)\n",
    "\n",
    "        sorted_distances = np.sort(distances[:, -1])\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(sorted_distances)\n",
    "        plt.xlabel(\"Points sorted by distance\")\n",
    "        plt.ylabel(f\"{k_values}-th Nearest Neighbor Distance\")\n",
    "        plt.title(\"k-Distance Graph to Find Optimal Eps\")\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9349ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that, it is acceptable for dbi between 1-2\n",
    "# because text data normally carry with high dimensions\n",
    "# silhouette score less preferable in text data\n",
    "\n",
    "def tune_dbscan(X, eps_values, min_samples_values):\n",
    "    \"\"\"\n",
    "    Tune DBSCAN parameters eps and min_samples by evaluating clustering quality.\n",
    "\n",
    "    Parameters:\n",
    "    - X: array-like, shape (n_samples, n_features)\n",
    "        Feature matrix for clustering.\n",
    "    - eps_values: iterable of float\n",
    "        List or range of epsilon values to try.\n",
    "    - min_samples_values: iterable of int\n",
    "        List or range of min_samples values to try.\n",
    "\n",
    "    Returns:\n",
    "    - best_eps: float\n",
    "        The epsilon value that yields the best Davies-Bouldin Index.\n",
    "    - best_score: float\n",
    "        The best Davies-Bouldin Index found.\n",
    "    - best_min_samples: int\n",
    "        The min_samples value that yields the best Davies-Bouldin Index.\n",
    "    \"\"\"\n",
    "\n",
    "    best_eps = None\n",
    "    best_score = float('inf')\n",
    "    best_min_samples = None\n",
    "\n",
    "    for min_samples in min_samples_values:\n",
    "        for eps in eps_values:\n",
    "            print(f\"Testing min_samples = {min_samples}, eps = {eps}\")\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1, metric= \"cosine\")\n",
    "            labels = dbscan.fit_predict(X)\n",
    "\n",
    "            print(\"Done Clustering\")\n",
    "\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            noise_points = list(labels).count(-1)\n",
    "\n",
    "            if n_clusters >= 2:\n",
    "\n",
    "                # exclude noise points\n",
    "                sil_score = silhouette_score(X[labels!= -1], labels[labels!=-1])\n",
    "                print(f\"Silhouette Coefficients: {sil_score:.4f}\")\n",
    "                db_index = davies_bouldin_score(X[labels!= -1], labels[labels!=-1])\n",
    "                print(f\"DBI: {db_index:.4f}\")\n",
    "                    \n",
    "                if db_index < best_score:\n",
    "                    best_score = db_index\n",
    "                    best_eps = eps\n",
    "                    best_min_samples = min_samples\n",
    "                \n",
    "                # how many clusters formed\n",
    "                print(\"Clustered labels: \", n_clusters)\n",
    "                # how many number of noise points\n",
    "                print(\"Noise points: \", noise_points)\n",
    "                \n",
    "            else:\n",
    "                print(\"No clusters formed or all points are noise.\")\n",
    "            \n",
    "            print()\n",
    "\n",
    "    return best_eps, best_score, best_min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_insert_labels(df, features, eps, min_samples, label_column_name):\n",
    "    \"\"\"\n",
    "    Apply DBSCAN clustering and insert cluster labels into the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame\n",
    "        Original dataframe to insert cluster labels.\n",
    "    - features: array-like, shape (n_samples, n_features)\n",
    "        Feature matrix used for clustering.\n",
    "    - eps: float\n",
    "        Epsilon parameter for DBSCAN.\n",
    "    - min_samples: int\n",
    "        Minimum samples parameter for DBSCAN.\n",
    "    - label_column_name: str\n",
    "        Name of the column to store cluster labels in the dataframe.\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas.DataFrame\n",
    "        DataFrame with added cluster labels.\n",
    "    \"\"\"\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1, metric='cosine')\n",
    "    labels = dbscan.fit_predict(features)\n",
    "\n",
    "    # insert labels into the original DataFrame\n",
    "    df[label_column_name] = labels\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    if n_clusters >= 2:\n",
    "\n",
    "        # exclude noise points\n",
    "        sil_score = silhouette_score(features[labels!=-1], labels[labels!=-1])\n",
    "        print(f\"Silhouette Coefficients: {sil_score:.4f}\")\n",
    "        db_index = davies_bouldin_score(features[labels!=-1], labels[labels!=-1])\n",
    "        print(f\"DBI: {db_index:.4f}\")\n",
    "\n",
    "        # how many clusters formed\n",
    "        print(\"Clustered labels: \", len(set(labels)) - 1)\n",
    "        # how many number of noise points\n",
    "        print(\"Noise points: \", list(labels).count(-1))\n",
    "        \n",
    "    else:\n",
    "        print(f\"[{label_column_name}] Too few clusters to compute DBI\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7116a",
   "metadata": {},
   "source": [
    "### **Positive Side Effects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(positive_side_effects, \"Positive Side Effects\", 200, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_side_effects = dimensionality_reduction(positive_side_effects, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963496f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(positive_side_effects, k = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce215fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_side_effects, eps_values = [0.025, 0.05, 0.075, 0.1, 0.15, 0.175, 0.2], min_samples_values = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_side_effects, eps_values = [0.025, 0.03, 0.035, 0.04], min_samples_values = [80, 90, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as min_samples=100 and eps=0.025 works better \n",
    "positive_reviews = cluster_and_insert_labels(\n",
    "    df=positive_reviews,\n",
    "    features=positive_side_effects,\n",
    "    eps=0.025,\n",
    "    min_samples=100,\n",
    "    label_column_name='side effects labels'\n",
    ")\n",
    "\n",
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b382c",
   "metadata": {},
   "source": [
    "### **Positive Effectiveness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18935f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(positive_side_effects, \"Positive Side Effects\", 200, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_effectiveness = dimensionality_reduction(positive_effectiveness, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce830ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(positive_effectiveness, k = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7315fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_effectiveness, eps_values = [0.05, 0.075, 0.095, 0.15, 0.175, 0.2], min_samples_values = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(positive_effectiveness, eps_values = [0.04, 0.045, 0.05, 0.055, 0.06], min_samples_values = [20, 30, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd714a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as min_samples=40 and eps=0.04 works better\n",
    "positive_reviews = cluster_and_insert_labels(\n",
    "    df=positive_reviews,\n",
    "    features=positive_effectiveness,\n",
    "    eps=0.04,\n",
    "    min_samples=40,\n",
    "    label_column_name='effectiveness labels'\n",
    ")\n",
    "\n",
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f79453",
   "metadata": {},
   "source": [
    "### **Negative Side Effects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(negative_side_effects, \"Negative Side Effects\", 200, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_side_effects = dimensionality_reduction(negative_side_effects, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(negative_side_effects, k = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_side_effects, eps_values = [0.025, 0.05, 0.075, 0.1, 0.15, 0.175, 0.2], min_samples_values = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ba284",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_side_effects, eps_values = [0.025, 0.03, 0.035, 0.04, 0.045], min_samples_values = [30, 70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = cluster_and_insert_labels(\n",
    "    df=negative_reviews,\n",
    "    features=negative_side_effects,\n",
    "    eps=0.025,\n",
    "    min_samples=30,\n",
    "    label_column_name='side effects labels'\n",
    ")\n",
    "\n",
    "print(\"The dimension of negative reviews:\", negative_reviews.shape)\n",
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c3f81",
   "metadata": {},
   "source": [
    "### **Negative Effectiveness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(negative_effectiveness, \"Negative Effectiveness\", 200, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_effectiveness = dimensionality_reduction(negative_effectiveness, 650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11faf482",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(negative_effectiveness, k = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2066119",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_effectiveness, eps_values = [0.05, 0.075, 0.1, 0.1125, 0.15, 0.175, 0.2, 0.225], min_samples_values = [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dbscan(negative_effectiveness, eps_values = [0.04, 0.045, 0.05, 0.055, 0.06, 0.065], min_samples_values = [20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = cluster_and_insert_labels(\n",
    "    df=negative_reviews,\n",
    "    features=negative_effectiveness,\n",
    "    eps=0.055,\n",
    "    min_samples=50,\n",
    "    label_column_name='effectiveness labels'\n",
    ")\n",
    "\n",
    "print(\"The dimension of negative reviews:\", negative_reviews.shape)\n",
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b2ace",
   "metadata": {},
   "source": [
    "## **Join Two Dataset Together**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews['sentiment'] = \"Positive\"\n",
    "negative_reviews['sentiment'] = \"Negative\"\n",
    "print(\"The dimension of positive reviews:\", positive_reviews.shape)\n",
    "print(\"The dimension of negative reviews:\", negative_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ff2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_reviews.isnull().sum().sum())\n",
    "print(negative_reviews.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246908f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ce7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the positive and negative reviews\n",
    "drug_reviews = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
    "drug_reviews = drug_reviews.reset_index(drop=True)\n",
    "\n",
    "# drop \"review\" column\n",
    "drug_reviews = drug_reviews.drop(columns=['review'])\n",
    "print(\"The dimension of drug reviews:\", drug_reviews.shape)\n",
    "drug_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_reviews.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "drug_reviews.to_csv('drug reviews embedding with labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
